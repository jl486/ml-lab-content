## Learning rate

A hyperparameter in optimization algorithms (such as gradient descent) that changes the step size per iteration as the algorithm steps towards a minimum of a loss function.

A high learning rate, the optimization algorithm will struggle with converging at a minimum. A low learning rate will cause training to take a long time.
