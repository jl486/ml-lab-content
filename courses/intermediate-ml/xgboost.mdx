---
id: xgboost
name: XGBoost
description: An efficient gradient boosting model.
---

Since the <Link href="/courses/intro-to-ml/random-forests">Introduction</Link> course, we've only discussed how to make predictions using <Link href="/glossary#random-forest">random forests</Link>, which predicts by aggregating the results of multiple decision trees trained on subsets of the dataset.

In this lesson, we'll try out another ensemble method called gradient boosting.

## Gradient boosting

<Link href="/glossary#gradient-boosting">Gradient boosting</Link> works by sequentially adding decision trees, with each one correcting the errors of the previous one.

The first step of the algorithm is to start with a constant value, which I'll call $f_0$. This value should **minimize** a <Link href="/glossary#loss-function">loss function</Link> like *squared error*. With some math we'll find that setting $f_0$ to the mean of the targets does just that.

Now, the process continues like so:

1. Start with a dataset $(\mathbf{X}, \mathbf{y})$. $\mathbf{X}$ represents all the **features** and $\mathbf{y}$ is the **target**.
2. Predictions from the model $f_1$ are compared to $\mathbf{y}$. Calculate the residuals.
3. The next model $f_2$ will use the previous residuals $\mathbf{r}_1$ as the target.
4. Repeat steps 2-3 above until all $M$ models are built and trained.
5. The entire model's prediction is $\hat{\mathbf{y}} = f_0 + \eta \cdot f_1(\mathbf{X}) + \cdots + \eta \cdot f_M(\mathbf{X})$, where $\eta$ is a constant called <Link href="/glossary#learning-rate">learning rate</Link>.

Here is a diagram of the algorithm we just described:

<Image src="/content/courses/02-intermediate-ml/assets/gradient-boosting.png" invert />

## XGBoost

XGBoost stands for *extreme gradient boosting*, and it is a library for extremely-optimized gradient boosting. XGBoost follows the same principles as gradient boosting but has tons of other optimizations to improve performance.

XGboost has a wrapper class called `XGBRegressor` that follows the scikit-learn interface, so we can simply use it like this:

```py
from sklearn.metrics import mean_absolute_error
import xgboost as xgb

model = xgb.XGBRegressor()
model.fit(X_train, y_train);

y_pred = model.predict(X_val)

print(mean_absolute_error(y_val, y_pred))
```

## Hyperparameters

Like decision trees or random forests, XGBoost has <Link href="/glossary#hyperparameter">hyperparameters</Link> that we can set. Hyperparameters are constants that change model behavior, like `max_leaf_nodes` which limits the number of leaf nodes of a decision tree. Here are the most important hyperparameters:

### Number of estimators

`n_estimators` is the number of trees that are generated by gradient boosting and are in the ensemble. A value from 100-1000 works well for most datasets. It's important to consider what you set for this hyperparameter because:

* a value too *low* can cause <Link href="/glossary#underfitting">underfitting</Link>.
* a value too *high* can cause <Link href="/glossary#overfitting">overfitting</Link>.

### Learning rate

`learning_rate` is a number from 0-1 that is multiplied with the prediction of each individual estimator before summing all the predictions (like in the equation from earlier):

$$
\hat{\mathbf{y}} = f_0 + \eta \cdot f_1(\mathbf{X}) + \eta \cdot f_2(\mathbf{X}) + \cdots + \eta \cdot f_M(\mathbf{X})
$$

Setting a low `learning_rate` and high `n_estimators` typically leads to better predictions, though this will take longer to train.

### Parallel threads

For large datasets, using **parallel threads** can help speed up the training process of your model, but for smaller datasets it won't have much of an impact.

The hyperparameter `n_jobs` sets the number of parallel threads to use during training. You usually would set this to the number of cores on your CPU or to `-1`, which means to use all available cores. By default, `n_jobs=1`, so it would only be using one CPU core.

### Tuning

It's your turn now! Tune `n_estimators` and `learning_rate` to minimize MAE on the Ames housing dataset. To start, try setting `n_estimators` to different values from `100`-`1000`, and `learning_rate` to values from `0.01`-`0.1`.

<MiniCollapsible>
  <MiniCollapsibleTrigger>See my values</MiniCollapsibleTrigger>
  <MiniCollapsibleContent>
    My hyperparameters were `n_estimators=220` and `learning_rate=0.076923`, which resulted in an MAE of `16255.24937928082`. Did you beat me, or do you think I'm underfitting/overfitting?

    If you're curious, I got these values by using a hyperparameter optimization library called <Link href="https://optuna.org/">Optuna</Link>.
  </MiniCollapsibleContent>
</MiniCollapsible>
